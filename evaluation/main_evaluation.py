import logging
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    answer_similarity,
    answer_correctness,
)
from rouge_score import rouge_scorer
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_vertexai import VertexAIEmbeddings
import torch
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate

from config import settings
from rag_core.agent import QueryAgent
import asyncio
from concurrent.futures import ThreadPoolExecutor
import os

# C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class BaselineAgent:
    """Agent cho ph∆∞∆°ng ph√°p baseline (zero-shot) - ch·ªâ s·ª≠ d·ª•ng LLM m√† kh√¥ng retrieve."""
    
    def __init__(self):
        self.llm = ChatGoogleGenerativeAI(
            model=settings.GENERATION_MODEL_PRO,
            google_api_key=settings.GOOGLE_API_KEY,
            temperature=0.1
        )
        
        # Prompt template cho zero-shot
        self.baseline_prompt = ChatPromptTemplate.from_messages([
            ("system", """B·∫°n l√† m·ªôt AI assistant th√¥ng minh. H√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt nh·∫•t c√≥ th·ªÉ d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n.

L∆∞u √Ω:
- N·∫øu b·∫°n kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th√¥ng tin, h√£y th√†nh th·∫≠t n√≥i r·∫±ng b·∫°n kh√¥ng bi·∫øt
- ƒê∆∞a ra c√¢u tr·∫£ l·ªùi c√≥ c·∫•u tr√∫c v√† d·ªÖ hi·ªÉu
- S·ª≠ d·ª•ng ti·∫øng Vi·ªát ƒë·ªÉ tr·∫£ l·ªùi"""),
            ("human", "{question}")
        ])
        
        self.chain = self.baseline_prompt | self.llm | StrOutputParser()
        logging.info("BaselineAgent ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o (Zero-shot)")
    
    def answer(self, question: str):
        """Tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ph∆∞∆°ng ph√°p zero-shot."""
        response = self.chain.invoke({"question": question})
        # Tr·∫£ v·ªÅ empty list cho contexts ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi format ƒë√°nh gi√°
        return response, []

def calculate_rouge_score(predictions, references):
    """T√≠nh to√°n ƒëi·ªÉm ROUGE-1."""
    from rouge_score import rouge_scorer
    scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)
    rouge_scores = []
    for pred, ref in zip(predictions, references):
        scores = scorer.score(ref, pred)
        rouge_scores.append(scores['rouge1'].fmeasure)
    return rouge_scores

def create_comparison_table(evaluation_results):
    """T·∫°o b·∫£ng so s√°nh k·∫øt qu·∫£ ƒë√°nh gi√° c·ªßa c√°c ph∆∞∆°ng ph√°p."""
    comparison_data = []
    
    for method, results in evaluation_results.items():
        avg_scores = {
            'Ph∆∞∆°ng ph√°p': method.upper(),
            'Answer Relevancy': f"{results['answer_relevancy'].mean():.4f}",
            'Context Recall': f"{results['context_recall'].mean():.4f}" if 'context_recall' in results else "N/A",
            'Faithfulness': f"{results['faithfulness'].mean():.4f}" if 'faithfulness' in results else "N/A", 
            'Answer Similarity': f"{results['answer_similarity'].mean():.4f}" if 'answer_similarity' in results else "N/A",
            'Answer Correctness': f"{results['answer_correctness'].mean():.4f}",
            'ROUGE-1': f"{results['rouge_score_1'].mean():.4f}"
        }
        comparison_data.append(avg_scores)
    
    comparison_df = pd.DataFrame(comparison_data)
    return comparison_df

async def run_evaluation_optimized():
    """Phi√™n b·∫£n t·ªëi ∆∞u v·ªõi batch processing v√† GPU."""
    
    
    logging.info("B·∫Øt ƒë·∫ßu quy tr√¨nh ƒë√°nh gi√° t·ªëi ∆∞u...")

    # LLM v·∫´n d√πng API
    gemini_llm_for_ragas = ChatGoogleGenerativeAI(
        model=settings.GENERATION_MODEL_PRO,
        google_api_key=settings.GOOGLE_API_KEY,
        temperature=0.1
    )
    ragas_gemini_llm = LangchainLLMWrapper(langchain_llm=gemini_llm_for_ragas)

    # Embedding d√πng GPU local ƒë·ªÉ tƒÉng t·ªëc
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model_kwargs = {'device': device, 'trust_remote_code': True}
    
    # T·ªëi ∆∞u batch size cho GPU
    encode_kwargs = {
        'batch_size': 64 if device == 'cuda' else 8,
        'device': device,
        'normalize_embeddings': True,
    }
    
    # S·ª≠ d·ª•ng embedding model local v·ªõi GPU
    gpu_embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",  # Model nh·∫π v√† nhanh
        model_kwargs=model_kwargs,
        encode_kwargs=encode_kwargs
    )
    ragas_gpu_embeddings = LangchainEmbeddingsWrapper(embeddings=gpu_embeddings)
    
    logging.info(f"Ragas embedding running on: {device}")

    try:
        qa_dataset = Dataset.from_json(str(settings.QA_DATASET_PATH))
        # Chuy·ªÉn ƒë·ªïi Dataset th√†nh dictionary of lists
        qa_dict = qa_dataset.to_dict()
        logging.info(f"ƒê√£ t·∫£i th√†nh c√¥ng {len(qa_dataset)} m·∫´u t·ª´ b·ªô d·ªØ li·ªáu ƒë√°nh gi√°.")
    except FileNotFoundError:
        logging.error(f"Kh√¥ng t√¨m th·∫•y file dataset t·∫°i: {settings.QA_DATASET_PATH}")
        return

    # Th√™m baseline v√†o danh s√°ch ph∆∞∆°ng ph√°p
    embedding_methods = list(settings.EMBEDDING_MODELS.keys()) + ['baseline']
    evaluation_results = {}  # Dictionary ƒë·ªÉ l∆∞u k·∫øt qu·∫£ c·ªßa t·∫•t c·∫£ ph∆∞∆°ng ph√°p

    for method in embedding_methods:
        logging.info(f"--- ƒêang ƒë√°nh gi√° ph∆∞∆°ng ph√°p: {method.upper()} ---")

        eval_data = {
            "question": [],
            "answer": [],
            "contexts": [],
            "ground_truth": qa_dict['ground_truth']  # S·ª≠ d·ª•ng qa_dict
        }

        # Kh·ªüi t·∫°o agent ph√π h·ª£p
        if method == 'baseline':
            agent = BaselineAgent()
            logging.info("S·ª≠ d·ª•ng BaselineAgent (Zero-shot)")
        else:
            agent = QueryAgent(embedding_method=method)
            logging.info(f"S·ª≠ d·ª•ng QueryAgent v·ªõi embedding: {method}")

        # T·∫°o list of dictionaries t·ª´ qa_dict
        qa_list = []
        for i in range(len(qa_dict['question'])):
            qa_list.append({
                'question': qa_dict['question'][i],
                'answer': qa_dict['answer'][i] if 'answer' in qa_dict else None,
                'contexts': qa_dict['contexts'][i] if 'contexts' in qa_dict else None,
                'ground_truth': qa_dict['ground_truth'][i]
            })

        # Batch processing v·ªõi ThreadPoolExecutor
        async def process_qa_batch(qa_batch):
            results = []
            for qa_pair in qa_batch:
                question = qa_pair['question']
                response, retrieved_docs = agent.answer(question)
                results.append((question, response, retrieved_docs))
            return results

        # Chia dataset th√†nh batches
        batch_size = 10 if torch.cuda.is_available() else 5
        qa_batches = [qa_list[i:i + batch_size] for i in range(0, len(qa_list), batch_size)]
        
        for i, batch in enumerate(qa_batches):
            logging.info(f"Processing batch {i+1}/{len(qa_batches)}")
            
            # X·ª≠ l√Ω batch
            batch_results = await process_qa_batch(batch)
            
            for question, response, retrieved_docs in batch_results:
                eval_data["question"].append(question)
                eval_data["answer"].append(response)
                # X·ª≠ l√Ω contexts cho baseline (empty list)
                if method == 'baseline':
                    eval_data["contexts"].append(["No context used (zero-shot)"])
                else:
                    eval_data["contexts"].append([doc.page_content for doc in retrieved_docs])
            
            # GPU memory management
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        eval_dataset = Dataset.from_dict(eval_data)

        # Ch·ªçn metrics ph√π h·ª£p cho t·ª´ng ph∆∞∆°ng ph√°p
        if method == 'baseline':
            # Baseline kh√¥ng c√≥ context, n√™n b·ªè qua context-related metrics
            metrics_to_evaluate = [
                answer_relevancy,
                answer_similarity,
                answer_correctness,
            ]
        else:
            metrics_to_evaluate = [
                answer_relevancy,
                context_recall,
                faithfulness,
                answer_similarity,
                answer_correctness,
            ]

        result = evaluate(
            dataset=eval_dataset,
            metrics=metrics_to_evaluate,
            llm=ragas_gemini_llm,
            embeddings=ragas_gpu_embeddings  # S·ª≠ d·ª•ng GPU embeddings
        )

        # Merge k·∫øt qu·∫£ metrics v·ªõi d·ªØ li·ªáu g·ªëc
        df_result_metrics = result.to_pandas()
        df_eval_data = pd.DataFrame(eval_data)
        df_result = pd.concat([df_eval_data, df_result_metrics], axis=1)

        # T√≠nh ROUGE-1
        logging.info("Calculating ROUGE-1 scores...")
        df_result['rouge_score_1'] = calculate_rouge_score(
            df_result['answer'], df_result['ground_truth']
        )

        # L∆∞u k·∫øt qu·∫£ ƒë·ªÉ so s√°nh sau n√†y
        evaluation_results[method] = df_result

        # Ch·ªâ l·∫•y c√°c c·ªôt quan tr·ªçng v√† rename
        if method == 'baseline':
            column_map = {
                "answer_relevancy": "Answer Relevancy (M·ª©c ƒë·ªô li√™n quan c·ªßa c√¢u tr·∫£ l·ªùi)",
                "answer_similarity": "Semantic Similarity (M·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a)",
                "rouge_score_1": "ROUGE-1 Score (ƒêi·ªÉm ROUGE-1)",
                "answer_correctness": "Answer Accuracy (M·ª©c ƒë·ªô ch√≠nh x√°c c·ªßa c√¢u tr·∫£ l·ªùi)"
            }
        else:
            column_map = {
                "answer_relevancy": "Answer Relevancy (M·ª©c ƒë·ªô li√™n quan c·ªßa c√¢u tr·∫£ l·ªùi)",
                "context_recall": "Context Recall (M·ª©c ƒë·ªô ƒë·∫ßy ƒë·ªß c·ªßa ng·ªØ c·∫£nh)",
                "faithfulness": "Faithfulness (M·ª©c ƒë·ªô trung th·ª±c c·ªßa c√¢u tr·∫£ l·ªùi)",
                "answer_similarity": "Semantic Similarity (M·ª©c ƒë·ªô t∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a)",
                "rouge_score_1": "ROUGE-1 Score (ƒêi·ªÉm ROUGE-1)",
                "answer_correctness": "Answer Accuracy (M·ª©c ƒë·ªô ch√≠nh x√°c c·ªßa c√¢u tr·∫£ l·ªùi)"
            }

        ordered_columns = [col for col in column_map.keys() if col in df_result.columns]
        df_result_ordered = df_result[ordered_columns].rename(columns=column_map)

        logging.info(f"K·∫øt qu·∫£ ƒë√°nh gi√° cho ph∆∞∆°ng ph√°p '{method}':\n{df_result_ordered}")

        output_path = settings.EVAL_RESULTS_DIR / f"evaluation_results_{method}.csv"
        df_result_ordered.to_csv(output_path, index=False)
        logging.info(f"ƒê√£ l∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° v√†o file: {output_path}")

    # T·∫°o b·∫£ng so s√°nh sau khi ƒë√°nh gi√° xong t·∫•t c·∫£ ph∆∞∆°ng ph√°p
    logging.info("\n" + "="*80)
    logging.info("T·∫†NG B·∫¢NG SO S√ÅNH K·∫æT QU·∫¢ ƒê√ÅNH GI√Å")
    logging.info("="*80)
    
    comparison_table = create_comparison_table(evaluation_results)
    print("\nüìä B·∫¢NG SO S√ÅNH ƒêI·ªÇM TRUNG B√åNH C√ÅC PH∆Ø∆†NG PH√ÅP:")
    print("="*100)
    print(comparison_table.to_string(index=False))
    print("="*100)
    
    # L∆∞u b·∫£ng so s√°nh
    comparison_path = settings.EVAL_RESULTS_DIR / "comparison_results.csv"
    comparison_table.to_csv(comparison_path, index=False)
    logging.info(f"ƒê√£ l∆∞u b·∫£ng so s√°nh v√†o file: {comparison_path}")

    logging.info("Quy tr√¨nh ƒë√°nh gi√° ƒë√£ ho√†n t·∫•t.")

def run_evaluation():
    """Wrapper ƒë·ªÉ ch·∫°y async evaluation."""
    asyncio.run(run_evaluation_optimized())

if __name__ == "__main__":
    run_evaluation()